<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>412project by KarlParkinson</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">412project</h1>
      <h2 class="project-tagline">CMPUT412 Winter 2016 Final Project.</h2>
      <a href="https://github.com/KarlParkinson/412Project" class="btn">View on GitHub</a>
      <a href="https://github.com/KarlParkinson/412Project/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/KarlParkinson/412Project/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="touch-screen-enabled-autonomous-robotic-vehicle" class="anchor" href="#touch-screen-enabled-autonomous-robotic-vehicle" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Touch Screen Enabled Autonomous Robotic Vehicle.</h3>

<p>The goal of this project was to create a differential drive robot that could be given task goals using an intuitive touchscreen interface and then complete the tasks autonomously using visual servoing. To complete this goal we designed a robot using Legoâ€™s EV3 bricks and created a touchscreen interface using a Android tablet. Visual servoing was accomplished using a Android phone as the camera, given 2DOF with pan and tilt motors. Additionally OpenCV for Android was used on the tablet for object detection. Furthermore Bluetooth was used for sending messages between the tablet and EV3. Finally we designed the robot to be able to go and grab a user selected ball and then take the ball to a user selected target and drop it.</p>

<p><img src="https://3.bp.blogspot.com/-5tna2I9cHPo/TV5XB85nTTI/AAAAAAAAAvY/uPnXQA8sxn8/s1600/cat-allergy.jpg" alt="cats"></p>

<p><img src="https://github.com/KarlParkinson/412Project/blob/master/robot1.jpg" alt=""></p>

<h3>
<a id="design" class="anchor" href="#design" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Design</h3>

<p>Our robot was created using the LEGO Mindstorms EV3 kit. It is a differential drive robot. The pan and tilt rig holding the camera has 2DOF and is able to move independently of the rest of the robot. The claw at the front is used for grasping objects, and the ultrasonic distance sensor is used to stop the robot when it reaches an object.</p>

<h3>
<a id="control" class="anchor" href="#control" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Control</h3>

<p>Task specification is done by a human operator through an Android touchscreen. The robot and the Android device then communicate over Bluetooth. OpenCV runs on the Android device to enable object tracking and visual servoing.</p>

<p><img src="https://github.com/KarlParkinson/412Project/blob/master/robot4.png" alt=""></p>

<p>The robot is able to autonomously control its own kinematics using visual servoing and simple geometry, as described in the accompanying report. The pan and tilt angles are adjusted to keep the object being tracked in the center of the image plane of the on-board camera. Using the pan and tilt angles, the angular velocity of the robot can be adjusted in order to intercept the target being tracked.</p>

<p><img src="https://github.com/KarlParkinson/412Project/blob/master/robot3.png" alt=""></p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>This project was created by Karl Parkinson (<a href="https://github.com/KarlParkinson" class="user-mention">@KarlParkinson</a>) and Sean Scheideman (<a href="https://github.com/scheidemanS" class="user-mention">@scheidemanS</a>) for the winter 2016 CMPUT 412: Experimental Mobile Robotics class at the University of Alberta.</p>

<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>

<p><a href="https://docs.google.com/document/d/14CSM05k-NNxKoMlzEd3SqhR6CNkV-cLqUes958Fc4g0">Report</a><br>
<a href="https://www.youtube.com/watch?v=oL15GbbHTNk">Video Demo</a><br>
<a href="http://www.dis.uniroma1.it/%7Elabrob/pub/papers/IROS05_Visual.pdf">http://www.dis.uniroma1.it/~labrob/pub/papers/IROS05_Visual.pdf</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/KarlParkinson/412Project">412project</a> is maintained by <a href="https://github.com/KarlParkinson">KarlParkinson</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
